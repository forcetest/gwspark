from pyspark import SparkContext, HiveContext
sc = SparkContext(appName = "test")
sqlContext = HiveContext(sc)
sqlContext.sql("ADD JAR hdfs:///spark/auxjar/geometry-api.jar")
sqlContext.sql("ADD JAR hdfs:///spark/auxjar/geowave-csv-driver-0.80.jar")
sqlContext.sql("ADD JAR hdfs:///spark/auxjar/geowave-driver-0.80.jar")
sqlContext.sql("ADD JAR hdfs:///spark/auxjar/geowave-geotools-datastore-0.80.jar")
sqlContext.sql("ADD JAR hdfs:///spark/auxjar/geowave-raster-driver-0.80.jar")
sqlContext.sql("ADD JAR hdfs:///spark/auxjar/geowave-share-0.80-jar-with-dependencies.jar")
sqlContext.sql("ADD JAR hdfs:///spark/auxjar/geowave-util-0.80.jar")
sqlContext.sql("ADD JAR hdfs:///spark/auxjar/geowave-vector-driver-0.80.jar")
sqlContext.sql("ADD JAR hdfs:///spark/auxjar/gt-api-16.0.jar")
sqlContext.sql("ADD JAR hdfs:///spark/auxjar/json-serde-1.3.6.jar")
sqlContext.sql("ADD JAR hdfs:///spark/auxjar/oushive.jar")
sqlContext.sql("ADD JAR hdfs:///spark/auxjar/spatial-sdk-hadoop.jar")
sqlContext.sql("set geowave.server.ip=j11.forcewave.co.kr")
sqlContext.sql("set geowave.server.port=54555")
df = sqlContext.sql("select gizscore from htmrres limit 100")
df.show()
